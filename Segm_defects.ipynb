{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import most of necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function for drawing mask polygons from list of x-y pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_mask_polygon(mask: np.ndarray, points: List, mask_color) -> np.ndarray:\n",
    "\t\"\"\"\n",
    "\t\tDraws a polygon of a segmented part specified by set of x-y coord pairs\n",
    "\n",
    "\tArgs:\n",
    "\t\tmask (np.ndarray): current mask to draw polygon on\n",
    "\t\tpoints (List): list of x-y coord pairs which define the polygon\n",
    "\t\tmask_color (int, Tuple): per-channel value for the polygon\n",
    "\n",
    "\tReturns:\n",
    "\t\tnp.ndarray: new mask with the polygon drawn\n",
    "\t\"\"\"\n",
    "\t# Parse sequence of x-y pairs\n",
    "\tobj_mask_points = []\n",
    "\tfor point in points:\n",
    "\t\tobj_mask_points.append([point[\"x\"], point[\"y\"]])\n",
    "\n",
    "\t# Draw polygon with specified color\n",
    "\treturn cv2.fillPoly(mask.copy(), pts=[np.array(obj_mask_points, dtype=np.int32)], color=mask_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset for storing images with the corresponding masks and some additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltDataset(Dataset):\n",
    "\tdef __init__(self, root_dir: str, data_dir: str, annot_file_name: str, transforms = None):\n",
    "\t\t\"\"\"\n",
    "\t\t\tBoltDataset default constructor\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\troot_dir (str): root directory where dataset is located \n",
    "\t\t\tdata_dir (str): directory with images and annnotations file\n",
    "\t\t\tannot_file_name (str): file name of an annotations file\n",
    "\t\t\ttransforms (_type_, optional): _description_. Defaults to None.\n",
    "\t\t\"\"\"\n",
    "\t\t# Pathes to all necessary data\n",
    "\t\tself.root_dir = root_dir\n",
    "\t\tself.data_dir = os.path.join(self.root_dir, data_dir)\n",
    "\t\tself.annot_file_path = os.path.join(self.data_dir, annot_file_name)\n",
    "\t\tself.transforms = transforms\n",
    "\t\t\n",
    "\t\t# Buffers for storing image and class info and image-mask pairs\n",
    "\t\tself.images_info = []\n",
    "\t\tself.images_with_masks = []\n",
    "\t\tself.id2class_categories = {0: \"background\"}\n",
    "\t\tself.class_categories2id = {\"background\": 0}\n",
    "\n",
    "\t\t# Load json annotations file\n",
    "\t\twith open(self.annot_file_path) as data_json:\n",
    "\t\t\tself.annot_data = json.load(data_json)\n",
    "\t\t\n",
    "\t\t# Parse class info\n",
    "\t\tfor category_info in self.annot_data[\"categories\"]:\n",
    "\t\t\tif(category_info[\"supercategory\"] is not None):\n",
    "\t\t\t\tself.id2class_categories[category_info[\"id\"]] = category_info[\"name\"]\n",
    "\t\t\t\tself.class_categories2id[category_info[\"name\"]] = category_info[\"id\"]\n",
    "\n",
    "\t\t# Parse image info\n",
    "\t\tfor image_info in self.annot_data[\"images\"]:\n",
    "\t\t\tself.images_info.append({\"file_name\": image_info[\"file_name\"], \"image_size\": (image_info[\"height\"], image_info[\"width\"])})\n",
    "\n",
    "\t\t# Parse annotations and apply transforms(if any)\n",
    "\t\tcurr_image_id = 0\n",
    "\t\tcurr_mask = np.zeros(self.images_info[curr_image_id][\"image_size\"], dtype=np.float32)\n",
    "\t\tbar = tqdm(enumerate(self.annot_data[\"annotations\"]), total=len(self.annot_data[\"annotations\"]))\n",
    "\t\tfor id, annot_info in bar:\n",
    "\t\t\tif annot_info[\"image_id\"] != curr_image_id or id == len(self.annot_data[\"annotations\"]) - 1:\n",
    "\t\t\t\timage = cv2.imread(os.path.join(self.data_dir, self.images_info[curr_image_id][\"file_name\"]))\n",
    "\t\t\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif(transforms is not None):\n",
    "\t\t\t\t\ttransformed = transforms(image=image, mask=curr_mask)\n",
    "\t\t\t\t\tself.images_with_masks.append((transformed[\"image\"], transformed[\"mask\"]))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.images_with_masks.append((torch.from_numpy(image).to(torch.float32).unsqueeze(0), torch.from_numpy(curr_mask).unsqueeze(0)))\n",
    "\n",
    "\t\t\t\tcurr_image_id = (curr_image_id + 1) % len(self.images_info)\n",
    "\t\t\t\tcurr_mask = np.zeros(self.images_info[curr_image_id][\"image_size\"], dtype=np.float32)\n",
    "\n",
    "\t\t\t\tbar.set_postfix_str(f\"Total processed images: {len(self.images_with_masks)}\")\n",
    "\t\t\t\n",
    "\t\t\tpoints = []\n",
    "\t\t\tfor point_id in range(0, len(annot_info[\"segmentation\"][0]), 2):\n",
    "\t\t\t\tpoints.append({\t\"x\": annot_info[\"segmentation\"][0][point_id], \n",
    "\t\t\t\t\t\t\t\t\"y\":annot_info[\"segmentation\"][0][point_id + 1]})\n",
    "\t\t\t\n",
    "\t\t\tcurr_mask = draw_mask_polygon(curr_mask, points, annot_info[\"category_id\"])\n",
    "\t\t\n",
    "\t\t### Algorithm for processing color images for further usage in dataset creation\n",
    "\n",
    "\t\t# \tmask = cv2.imread(os.path.join(self.masks_dir, image_name))\n",
    "\t\t# \tif(mask is None):\n",
    "\t\t# \t\tprint(f\"Warning:{image_name} does not have associated mask. Skipping...\")\n",
    "\t\t# \t\tcontinue\n",
    "\n",
    "\t\t# \tself.image_names.append(image_name)\n",
    "\n",
    "\t\t# \timage = cv2.bitwise_not(image)\n",
    "\t\t# \timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t\t# \timage = cv2.multiply(image[:,:,0], (1,1), scale=0.7)\n",
    "\n",
    "\t\t# \tcv2.imwrite(os.path.join(self.masks_dir, image_name), image)\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\" \n",
    "\t\t\tGetting number of dataset elements\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self.images_with_masks)\n",
    "\n",
    "\tdef __getitem__(self, idx: int):\n",
    "\t\t\"\"\" \n",
    "\t\t\tGetting particular element of a dataset via index\n",
    "\t\t\n",
    "\t\tArguments:\n",
    "\t\t\tidx (int): index of a needed element\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.images_with_masks[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display particular sample from the created dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sample(dataset: BoltDataset, idx: int, class_colors: Dict, pred_masks_dir: str = None, pred_mask: torch.Tensor = None, alpha: float = 0.2) -> List[np.ndarray]:\n",
    "\t\"\"\" Merges provided by index from dataset image with either predicted or loaded mask\n",
    "\n",
    "\tArgs:\n",
    "\t\tdataset (BoltDataset): dataset to take image by index from\n",
    "\t\tidx (int): index of image to merge with mask\n",
    "\t\tclass_colors (Dict): dictionary of RGB colors(tuples) for each category on a mask\n",
    "\t\tpred_masks_dir (str, optional): path to a predicted mask. Defaults to None.\n",
    "\t\tpred_mask (torch.Tensor, optional): tensor of predicted mask. Defaults to None.\n",
    "\t\talpha (float, optional): transparency of a mask. Defaults to 0.2.\n",
    "\n",
    "\tReturns:\n",
    "\t\tList[np.ndarray]: original image with applied mask on top and the mask itself\n",
    "\t\"\"\"\n",
    "\n",
    "\tmask = None\n",
    "\tif(pred_masks_dir is not None):\n",
    "\t\t# When the mask is taken on specifed path\n",
    "\t\ttry:\n",
    "\t\t\tmask = cv2.imread(os.path.join(os.path.join(dataset.root_dir, pred_masks_dir), dataset.images_info[idx][\"file_name\"]))\n",
    "\t\texcept IndexError:\n",
    "\t\t\tprint(\"There is no image with such index in the dataset.\")\n",
    "\t\t\treturn None\n",
    "\t\n",
    "\t\tassert mask is not None, \"There is no mask for this image. Check mask name(it has to be the same with image) or perform segmentation to obtain the it.\"\n",
    "\telse:\n",
    "\t\tif(pred_mask is None):\n",
    "\t\t\t# When the mask is taken from dataset\n",
    "\t\t\tmask = np.repeat(dataset[idx][1].permute(1, 2, 0).numpy(), 3, axis=2).astype(np.uint8)\n",
    "\t\telse:\n",
    "\t\t\t# When the mask is provided(usually it is postprocessed mask)\n",
    "\t\t\tmask = np.repeat(pred_mask.permute(1, 2, 0).numpy(), 3, axis=2).astype(np.uint8)\n",
    "\t\t\n",
    "\t\t# Coloring the mask\n",
    "\t\tfor class_id in class_colors.keys():\n",
    "\t\t\tfor channel_id in range(3):\n",
    "\t\t\t\tmask[:, :, channel_id] = np.where(mask[:, :, channel_id] == class_id, class_colors[class_id][channel_id], mask[:, :, channel_id])\n",
    "\n",
    "\t# Image and mask postprocessing\n",
    "\timage = dataset[idx][0].permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\tmask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "\treturn cv2.addWeighted(image, 1.0, mask, alpha, 0), mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pathes to [the downloaded dataset](https://universe.roboflow.com/arios-workspace/segmtest/dataset/1) parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [00:01<00:00, 537.26it/s, Total processed images: 285]\n",
      "100%|██████████| 108/108 [00:00<00:00, 504.61it/s, Total processed images: 27]\n",
      "100%|██████████| 57/57 [00:00<00:00, 599.17it/s, Total processed images: 13]\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"roboflow_dataset\"\n",
    "train_images_dir, val_images_dir, test_images_dir = \"train\", \"valid\", \"test\" \n",
    "annot_file_path = \"_annotations.coco.json\"\n",
    "\n",
    "train_dataset = BoltDataset(root_dir=root_dir, data_dir=train_images_dir, annot_file_name=annot_file_path)\n",
    "val_dataset = BoltDataset(root_dir=root_dir, data_dir=val_images_dir, annot_file_name=annot_file_path)\n",
    "test_dataset = BoltDataset(root_dir=root_dir, data_dir=test_images_dir, annot_file_name=annot_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a particular sample from a particular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {1: [0, 250, 125], 2: [250, 0, 125]}\n",
    "image_IDX = 10\n",
    "\n",
    "cv2.imshow('Masked', mask_sample(test_dataset, image_IDX, colors)[0])\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom model with `segmentation_models_pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for training and validating the model(can be merged in one function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, device: str):\n",
    "\t# Buffer for calculating epoch metrics and total loss\n",
    "\tmetrics = {\"tp\": [], \"fp\": [], \"tn\": [], \"fn\": []}\n",
    "\ttotal_loss = 0.0\n",
    "\n",
    "\tmodel.train()\n",
    "\tbar = tqdm(train_loader)\n",
    "\tfor _, (images, target) in enumerate(bar, start=1):\n",
    "\t\timages = images.to(device, non_blocking=True)\n",
    "\t\ttarget = target.to(device, non_blocking=True).squeeze(1).long() - 1\n",
    "\t\toutput = model(images)\n",
    "\t\t\n",
    "\t\t# Treat low probabilities as 0(value - hyperparameter)\n",
    "\t\toutput = torch.where(output.ge(0.4), output, .0)\n",
    "\t\t\n",
    "\t\tloss = criterion(output, target)\n",
    "\t\ttotal_loss += loss.item()\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ttp, fp, fn, tn = smp.metrics.get_stats(output.argmax(dim=1).long(), target, mode='multiclass', num_classes=3, ignore_index=-1)\n",
    "\t\t\tmetrics[\"tp\"].append(tp)\n",
    "\t\t\tmetrics[\"fp\"].append(fp)\n",
    "\t\t\tmetrics[\"tn\"].append(tn)\n",
    "\t\t\tmetrics[\"fn\"].append(fn)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tbar.set_postfix_str(f\"Train / Epoch: {epoch}. Loss {total_loss / _}\")\n",
    "\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\treturn \ttorch.cat(metrics[\"tp\"], dim=0),\\\n",
    "\t\t\ttorch.cat(metrics[\"fp\"], dim=0),\\\n",
    "\t\t\ttorch.cat(metrics[\"fn\"], dim=0),\\\n",
    "\t\t\ttorch.cat(metrics[\"tn\"], dim=0)\n",
    "\t\t\n",
    "def validate(val_loader, model, criterion, epoch, device: str):\n",
    "\t# Buffer for calculating epoch metrics and total loss\n",
    "\tmetrics = {\"tp\": [], \"fp\": [], \"tn\": [], \"fn\": []}\n",
    "\ttotal_loss = 0.0\n",
    "\n",
    "\tmodel.eval()\n",
    "\tbar = tqdm(val_loader)\n",
    "\twith torch.no_grad():\n",
    "\t\tfor _, (images, target) in enumerate(bar, start=1):\n",
    "\t\t\timages = images.to(device, non_blocking=True)\n",
    "\t\t\ttarget = target.to(device, non_blocking=True).squeeze(1).long() - 1\n",
    "\t\t\toutput = model(images)\n",
    "\n",
    "\t\t\t# Treat low probabilities as 0(value - hyperparameter)\n",
    "\t\t\toutput = torch.where(output.ge(0.4), output, .0)\n",
    "\t\t\t\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\n",
    "\t\t\tbar.set_postfix_str(f\"Validation / Epoch: {epoch}. Loss{total_loss / _}\")\n",
    "\n",
    "\t\t\ttp, fp, fn, tn = smp.metrics.get_stats(output.argmax(dim=1).long(), target, mode='multiclass', num_classes=3, ignore_index=-1)\n",
    "\t\t\tmetrics[\"tp\"].append(tp)\n",
    "\t\t\tmetrics[\"fp\"].append(fp)\n",
    "\t\t\tmetrics[\"tn\"].append(tn)\n",
    "\t\t\tmetrics[\"fn\"].append(fn)\n",
    "\t\t\t\n",
    "\t\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\treturn \ttorch.cat(metrics[\"tp\"], dim=0),\\\n",
    "\t\t\ttorch.cat(metrics[\"fp\"], dim=0),\\\n",
    "\t\t\ttorch.cat(metrics[\"fn\"], dim=0),\\\n",
    "\t\t\ttorch.cat(metrics[\"tn\"], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and training segmentation model\n",
    "`Best found backbone options are:` `se_resnext50_32x4d`, `resnetN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# In case of error about expired ssl certificate\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Creating dataloaders\n",
    "train_loader = DataLoader(\n",
    "\t\ttrain_dataset,\n",
    "\t\tbatch_size = 4,\n",
    "\t\tshuffle = True,\n",
    "\t\tpin_memory=True,\n",
    "\t)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\tval_dataset,\n",
    "\tbatch_size = 4,\n",
    "\tshuffle = False,\n",
    "\tpin_memory = True,\n",
    ")\n",
    "\n",
    "# Specify \"cuda\" as device to train model on(if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Some hyperparameters\n",
    "lr=2e-4\n",
    "num_epochs = 10\n",
    "\n",
    "# Creating model with paticular architecture and backbone\n",
    "model = smp.create_model(\n",
    "\t\t\tarch=\"unet\",\n",
    "\t\t\tencoder_name=\"se_resnext50_32x4d\",\n",
    "\t\t\tin_channels=1,\n",
    "\t\t\tclasses=2,\n",
    "            activation=\"sigmoid\"\n",
    "\t\t).to(device)\n",
    "\n",
    "# Loss criterion and optimizer\n",
    "criterion = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, from_logits=False, log_loss=True, ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training loop for the defined number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [01:01<00:00,  1.17it/s, Train / Epoch: 1. Loss 1.07827548806866]  \n",
      "100%|██████████| 7/7 [00:03<00:00,  1.91it/s, Validation / Epoch: 1. Loss0.5456941212926593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.866, IoU(dataset): 0.852, Precision(imagewise): 0.92, Precision(dataset): 0.92, Recall(imagewise): 0.92, Recall(dataset): 0.92\n",
      "Validation IoU(imagewise): 0.971, IoU(dataset): 0.973, Precision(imagewise): 0.98, Precision(dataset): 0.99, Recall(imagewise): 0.98, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:38<00:00,  1.85it/s, Train / Epoch: 2. Loss 0.3926733715666665] \n",
      "100%|██████████| 7/7 [00:01<00:00,  6.83it/s, Validation / Epoch: 2. Loss0.5621372929641179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.972, IoU(dataset): 0.973, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.962, IoU(dataset): 0.965, Precision(imagewise): 0.98, Precision(dataset): 0.98, Recall(imagewise): 0.98, Recall(dataset): 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:38<00:00,  1.85it/s, Train / Epoch: 3. Loss 0.3095161273247666] \n",
      "100%|██████████| 7/7 [00:01<00:00,  6.89it/s, Validation / Epoch: 3. Loss0.36857698219163076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.973, IoU(dataset): 0.974, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.973, IoU(dataset): 0.976, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:39<00:00,  1.84it/s, Train / Epoch: 4. Loss 0.24172176989830202]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.80it/s, Validation / Epoch: 4. Loss0.3562841628279005] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.977, IoU(dataset): 0.979, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.974, IoU(dataset): 0.976, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:39<00:00,  1.83it/s, Train / Epoch: 5. Loss 0.18873227566170195]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.75it/s, Validation / Epoch: 5. Loss0.33380840931619915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.979, IoU(dataset): 0.981, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.975, IoU(dataset): 0.976, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:39<00:00,  1.82it/s, Train / Epoch: 6. Loss 0.16455889493227005]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.72it/s, Validation / Epoch: 6. Loss0.3440100465502058] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.982, IoU(dataset): 0.983, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.975, IoU(dataset): 0.977, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:39<00:00,  1.83it/s, Train / Epoch: 7. Loss 0.16276814591967398]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.72it/s, Validation / Epoch: 7. Loss0.3044664242437908] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.983, IoU(dataset): 0.984, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.976, IoU(dataset): 0.978, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:39<00:00,  1.81it/s, Train / Epoch: 8. Loss 0.13869249375743997]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.72it/s, Validation / Epoch: 8. Loss0.3531170295817511] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.985, IoU(dataset): 0.986, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.975, IoU(dataset): 0.977, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:39<00:00,  1.83it/s, Train / Epoch: 9. Loss 0.13497281726449728]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.84it/s, Validation / Epoch: 9. Loss0.3164951375552586] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.985, IoU(dataset): 0.986, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.975, IoU(dataset): 0.978, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:39<00:00,  1.82it/s, Train / Epoch: 10. Loss 0.11836361600500014]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.73it/s, Validation / Epoch: 10. Loss0.30814724947725025]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU(imagewise): 0.986, IoU(dataset): 0.987, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n",
      "Validation IoU(imagewise): 0.976, IoU(dataset): 0.978, Precision(imagewise): 0.99, Precision(dataset): 0.99, Recall(imagewise): 0.99, Recall(dataset): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "\ttrain_tp, train_fp, train_fn, train_tn = train(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\tval_tp, val_fp, val_fn, val_tn = validate(val_loader, model, criterion, epoch, device)\n",
    "\n",
    "\ttrain_imagewise_iou = smp.metrics.iou_score(train_tp, train_fp, train_fn, train_tn, reduction=\"micro-imagewise\")\n",
    "\ttrain_dataset_iou = smp.metrics.iou_score(train_tp, train_fp, train_fn, train_tn, reduction=\"micro\")\n",
    "\ttrain_imagewise_prec = smp.metrics.precision(train_tp, train_fp, train_fn, train_tn, reduction=\"micro-imagewise\")\n",
    "\ttrain_dataset_prec = smp.metrics.precision(train_tp, train_fp, train_fn, train_tn, reduction=\"micro\")\n",
    "\ttrain_imagewise_rec = smp.metrics.recall(train_tp, train_fp, train_fn, train_tn, reduction=\"micro-imagewise\")\n",
    "\ttrain_dataset_rec = smp.metrics.recall(train_tp, train_fp, train_fn, train_tn, reduction=\"micro\")\n",
    "\n",
    "\tval_imagewise_iou = smp.metrics.iou_score(val_tp, val_fp, val_fn, val_tn, reduction=\"micro-imagewise\")\n",
    "\tval_dataset_iou = smp.metrics.iou_score(val_tp, val_fp, val_fn, val_tn, reduction=\"micro\")\n",
    "\tval_imagewise_prec = smp.metrics.precision(val_tp, val_fp, val_fn, val_tn, reduction=\"micro-imagewise\")\n",
    "\tval_dataset_prec = smp.metrics.precision(val_tp, val_fp, val_fn, val_tn, reduction=\"micro\")\n",
    "\tval_imagewise_rec = smp.metrics.recall(val_tp, val_fp, val_fn, val_tn, reduction=\"micro-imagewise\")\n",
    "\tval_dataset_rec = smp.metrics.recall(val_tp, val_fp, val_fn, val_tn, reduction=\"micro\")\n",
    "\n",
    "\tprint(f\"Train IoU(imagewise): {train_imagewise_iou:.3f}, IoU(dataset): {train_dataset_iou:.3f}, Precision(imagewise): {train_imagewise_prec:.2f}, Precision(dataset): {train_dataset_prec:.2f}, Recall(imagewise): {train_imagewise_rec:.2f}, Recall(dataset): {train_dataset_rec:.2f}\")\n",
    "\tprint(f\"Validation IoU(imagewise): {val_imagewise_iou:.3f}, IoU(dataset): {val_dataset_iou:.3f}, Precision(imagewise): {val_imagewise_prec:.2f}, Precision(dataset): {val_dataset_prec:.2f}, Recall(imagewise): {val_imagewise_rec:.2f}, Recall(dataset): {val_dataset_rec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment a particular image from the test dataset with a curtain level of confidence for the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_postprocess_with_conf(mask: torch.Tensor, confidence: float = 0.9) -> np.ndarray:\n",
    "\t\"\"\" Postprocess a mask with per class confidence level. If the confidence for both classes of a pixel are less\n",
    "\t\tthan specified level then it is classified as \"background\" \n",
    "\n",
    "\tArgs:\n",
    "\t\tmask (torch.Tensor): predicted mask\n",
    "\t\tconfidence (float, optional): confidence level. Defaults to 0.9.\n",
    "\n",
    "\tReturns:\n",
    "\t\tnp.ndarray: postprocessed mask\n",
    "\t\"\"\"\n",
    "\tneg_conf_mask = torch.where(mask.max(dim=1)[0].lt(confidence), -1, 0)\n",
    "\tconf_class_mask = torch.where(mask.ge(confidence), mask, 0).argmax(dim=1)\n",
    "\tconf_mask = conf_class_mask + neg_conf_mask + 1\n",
    "\n",
    "\treturn conf_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment a particular image from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 169573, 1: 90279, 2: 2292}\n"
     ]
    }
   ],
   "source": [
    "image_IDX = 0\t# Index of image from test dataset to segment\n",
    "conf = 0.95\t\t# Confidence level\n",
    "\n",
    "# Processing image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mask = model(test_dataset[image_IDX][0].unsqueeze(0).to(device)).cpu()\n",
    "\n",
    "# Preprocessing the mask\n",
    "conf_mask = mask_postprocess_with_conf(mask, conf)\n",
    "\n",
    "# Debug info(can be commented out)\n",
    "unique, counts = np.unique(conf_mask.numpy(), return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Show masked image\n",
    "cv2.imshow('Masked', mask_sample(test_dataset, image_IDX, colors, pred_mask=conf_mask)[0])\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save segmentation masks and masked images for all samples from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  8.08it/s]\n"
     ]
    }
   ],
   "source": [
    "masked_images_dir = \"masked\"\t# Directory where to save masked images\n",
    "pred_masks_dir = \"pred_masks\"\t# Directory where to save masks\n",
    "conf = 0.95\t\t\t\t\t\t# Confidence level for classes\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\tbar = tqdm(range(len(test_dataset)))\n",
    "\tfor image_ID in bar:\n",
    "\t\t# Segmenting each image from test dataset\n",
    "\t\tpred_mask = model(test_dataset[image_ID][0].unsqueeze(0).to(device)).cpu()\n",
    "\t\t\n",
    "\t\t# Preprocessing each mask\n",
    "\t\tconf_mask = mask_postprocess_with_conf(pred_mask, conf)\n",
    "\n",
    "\t\t# Getting masked image and colored mask\n",
    "\t\tmasked_image, conf_mask = mask_sample(test_dataset, image_ID, colors, pred_mask=conf_mask)\t\n",
    "\n",
    "\t\t# And finally save everything\n",
    "\t\tcv2.imwrite(os.path.join(os.path.join(root_dir, masked_images_dir), test_dataset.images_info[image_ID][\"file_name\"]), masked_image)\n",
    "\t\tcv2.imwrite(os.path.join(os.path.join(root_dir, pred_masks_dir), test_dataset.images_info[image_ID][\"file_name\"]), conf_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Roboflow API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to a curtain project version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "# Private API key\n",
    "API_KEY = # YOUR_API_KEY\n",
    "\n",
    "# Connecting to a particular version of the project\n",
    "rf = Roboflow(api_key=API_KEY)\n",
    "project = rf.workspace().project(\"segmtest\")\n",
    "rf_model = project.version(1).model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment all images from the test dataset and save them with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:12<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "masked_images_dir = \"masked\"\t# Directory where to save masked images\n",
    "pred_masks_dir = \"pred_masks\"\t# Directory where to save masks\n",
    "\n",
    "bar = tqdm(range(len(test_dataset)))\n",
    "for image_ID in bar:\n",
    "\t# Segmenting each image from test dataset with curtain level of confidence\n",
    "\tpred_results = rf_model.predict(os.path.join(os.path.join(test_dataset.data_dir, test_dataset.images_info[image_ID][\"file_name\"])), confidence=40).json()[\"predictions\"]\n",
    "\n",
    "\t# Creating empty mask with the shape of [IMAGE_HEIGHT, IMAGE_WIDTH, 3]\n",
    "\timage_mask = np.zeros(test_dataset[image_ID][0].permute(1, 2, 0).numpy().shape, dtype=np.uint8)\n",
    "\t\n",
    "\tfor pred in pred_results:\n",
    "\t\timage_mask = draw_mask_polygon(image_mask, pred[\"points\"], test_dataset.class_categories2id[pred[\"class\"]])\n",
    "\n",
    "\tmasked_image, mask = mask_sample(test_dataset, image_ID, colors, pred_mask=torch.from_numpy(image_mask).permute(2, 0, 1))\n",
    "\n",
    "\tcv2.imwrite(os.path.join(os.path.join(root_dir, masked_images_dir), test_dataset.images_info[image_ID][\"file_name\"]), masked_image)\n",
    "\tcv2.imwrite(os.path.join(os.path.join(root_dir, pred_masks_dir), test_dataset.images_info[image_ID][\"file_name\"]), mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_Segm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
